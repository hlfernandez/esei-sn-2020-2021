{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative filtering using the Surprise library\n",
    "\n",
    "Surprise (http://surpriselib.com/) es una librería Python para crear y analizar sistemas de recomendación colaborativos utilizando datos de preferencia explícitos. Surprise incorpora implementaciones de los algoritmos más utilizados, datasets de prueba ([Movielens](http://grouplens.org/datasets/movielens/), [Jester](http://eigentaste.berkeley.edu/dataset/)), y herramientas para facilitarlla evaluación y comparación de distintos algoritmos y parámetros de los mismos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uso básico de Surprise\n",
    "\n",
    "El siguiente ejemplo muestra el uso básico de la librería con un dataset con las preferencias de 5 usuarios (A, B, C, D y E) sobre 2 ítems (salvo para el usuario E del que solo se dispone de su voto para el ítem 1). Se emplea un algoritmo basado en vecinos cercanos (el que vimos en teoría en el apartado `Collaborative filtering`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "\n",
    "ratings_dict = {\n",
    "    \"item\": [1, 2, 1, 2, 1, 2, 1, 2, 1],\n",
    "    \"user\": ['A', 'A', 'B', 'B', 'C', 'C', 'D', 'D', 'E'],\n",
    "    \"rating\": [1, 2, 2, 4, 2.5, 4, 4.5, 5, 3],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(ratings_dict)\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "data = Dataset.load_from_df(df[[\"user\", \"item\", \"rating\"]], reader)\n",
    "\n",
    "# We will use the \"KNNWithMeans\", a k-NN based algorithm.\n",
    "\n",
    "from surprise import KNNWithMeans\n",
    "\n",
    "# Configuration to use item-based cosine similarity\n",
    "\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # Compute  similarities between items\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "trainingSet = data.build_full_trainset()\n",
    "\n",
    "algo.fit(trainingSet)\n",
    "\n",
    "# Make a prediction for user E\n",
    "\n",
    "prediction = algo.predict('E', 2)\n",
    "print('\\nPrediction for E (item-based): ')\n",
    "display(prediction.est)\n",
    "\n",
    "# Configuration to use user-based cosine similarity\n",
    "\n",
    "sim_options = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": True,  # Compute  similarities between items\n",
    "}\n",
    "algo = KNNWithMeans(sim_options=sim_options)\n",
    "\n",
    "trainingSet = data.build_full_trainset()\n",
    "\n",
    "algo.fit(trainingSet)\n",
    "\n",
    "# Make a prediction for user E\n",
    "\n",
    "prediction = algo.predict('E', 2)\n",
    "print('\\nPrediction for E (user-based): ')\n",
    "display(prediction.est)\n",
    "\n",
    "# Alternatively, we can predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainingSet.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "display(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando el dataset MovieLens 100K\n",
    "\n",
    "Este ejemplo, sacado de la [documentación de Surprise](https://surprise.readthedocs.io/en/stable/FAQ.html), muestra cómo obtener las top N recomendaciones para cada usuario (i.e. las predicciones más altas para ítems que los usuarios no tenían en sus preferencias) utilizando el [dataset MovieLens 100K](https://grouplens.org/datasets/movielens/100k/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "\n",
    "\n",
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)\n",
    "\n",
    "# Print the recommended items for each user\n",
    "for uid, user_ratings in top_n.items():\n",
    "    print(uid, [iid for (iid, _) in user_ratings])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Buscando los mejores parámetros de cada algoritmo\n",
    "\n",
    "La librería surprise incluye una función llamada `GridSearchCV` que evalúa un conjunto de posibles parámetros para un algoritmo y nos devuelve la mejor combinacion de los mismos (en base a una métrica data, como puede ser el RMSE que vimos en clase de teoría).\n",
    "\n",
    "Utilizando el dataset MovieLens 100K, este primer ejemplo evalúa, para un algoritmo basado en vecinos cercanos, los mejores valores de tres parámetros: métrica de distancia (`name`), el número mínimo de items en común (`min_support`), y se debe aplicar `user-based` o `item-based`.\n",
    "\n",
    "*Nota*: este ejemplo puede tardar unos cuantos minutos en ejecutarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNWithMeans\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "sim_options = {\n",
    "    \"name\": [\"msd\", \"cosine\"],\n",
    "    \"min_support\": [3, 4, 5],\n",
    "    \"user_based\": [False, True],\n",
    "}\n",
    "\n",
    "param_grid = {\"sim_options\": sim_options}\n",
    "\n",
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a estos resultados, vemos que el mejor resultado para este algoritmo se obtiene utilizando un enfoque `item-based`, con un soporte mínimo de 3 y la métrica `msd`.\n",
    "\n",
    "Evaluamos ahora un enfoque basado en `matrix factorization`, implementado en Surprise en la función `SVD` (entre otras). En este caso, evaluamos tres parámetros: el número de épocas (o iteraciones sobre el dataset completo) del algoritmo de optimización (`n_epochs`), la tasa de aprendizaje (`lr_all`) que indica cómo de grandes deben ser los ajustes en los parámetros (los valores en descomposición en matrices), y `reg_all`, que es una penalización que se añade para evitar el sobre-entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-100k\")\n",
    "\n",
    "param_grid = {\n",
    "    \"n_epochs\": [5, 10],\n",
    "    \"lr_all\": [0.002, 0.005],\n",
    "    \"reg_all\": [0.4, 0.6]\n",
    "}\n",
    "gs = GridSearchCV(SVD, param_grid, measures=[\"rmse\", \"mae\"], cv=3)\n",
    "\n",
    "gs.fit(data)\n",
    "\n",
    "print(gs.best_score[\"rmse\"])\n",
    "print(gs.best_params[\"rmse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se puede ver que el mejor resultado se obtiene con 10 épocas, una tasa de aprendizaje de 0.005 y una regularización de 0.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecturas prácticas sobre collaborative filtering\n",
    "\n",
    "Al terminar este notebook, se recomienda la lectura de la siguiente entrada de blog en la que se repasan los conceptos básicos del collaborative filtering y se incluyen ejemplos (tomados para este notebook) utilizando Surprise: [Build a Recommendation Engine With Collaborative Filtering](https://realpython.com/build-recommendation-engine-collaborative-filtering/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
