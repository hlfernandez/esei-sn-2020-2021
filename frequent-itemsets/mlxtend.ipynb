{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mining frequent itemsets and association rules with `mlxtend`\n",
    "\n",
    "En este notebook utilizaremos mlxtend (http://rasbt.github.io/mlxtend/) y pandas (https://pandas.pydata.org/9) para buscar frequent itemsets y reglas de asociación. Comenzamos importando los paquetes necesarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori\n",
    "\n",
    "En primer lugar, utilizaremos el algoritmo Apriori. La función `apriori` (http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/apriori/) de esta librería espera los datos de las transacciones en un dataframe pandas con una \"one-hot encoding\". Supongamos que tenemos la siguiente lista de transacciones (lista de listas):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "           ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "           ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "           ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos transformar esta lista de listas a un dataframe pandas con la codificación adecuada utilizando el `TransactionEncoder`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar el análisis, vamos a obtener una gráfica con los 5 productos más frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis=0).sort_values(ascending=False)[0:5].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a buscar los itemsets con un soporte mínimo del 60%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import apriori\n",
    "\n",
    "apriori(df, min_support=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, la función devuelve los índices de las columnas de los ítems, lo cual puede ser útil para análisis posteriores automáticos. Sin embargo, para mejora rla legibilidad, podemos utilizar `use_colnames=True` y obtener los nombres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apriori(df, min_support=0.6, use_colnames=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El resultado de la librería es un DataFrame de pandas, que podemos utilizar para filtrar los resultados fácilmente. Por ejemplo, vamos a quedarnos solo con los itemsets de longitud 2 con un soporte mínimo del 80%. Primero, creamos los frequent itemsets con `apriori` y luego le añadimos una columna al DataFrame con la longitud de cada itemset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora podemos filtrar el resultado fácilmente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets[ (frequent_itemsets['length'] == 2) &\n",
    "                   (frequent_itemsets['support'] >= 0.8) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reglas de asociación\n",
    "\n",
    "Ahora que tenemos los frequent itemsets, podemos descubrir reglas de asociación a partir de ellos. La función `generate_rules` de la librería (http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/association_rules/) toma como entrada el DataFrame de frequent itemsets producido por la función `apriori`. La función `generate_rules` permite espefificar la métrica que se quiere emplear (confidence, lift, etc.) y el umbral correspondiente. Vamos a obtener las reglas cuyo nivel de confianza esté por encima del 70%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "frequent_itemsets = apriori(df, min_support=0.6, use_colnames=True)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y ahora, vamos a quedarnos con las reglas con un lift de al menos 1.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar los filtros de los DataFrames de pandas para refinar los resultados. Supongamos que queremos reglas con: \n",
    " - Por lo menos 2 antecedentes.\n",
    " - Una confianza superior al 75%.\n",
    " - Un lift > 1.2.\n",
    "\n",
    "En primer lugar, añadimos al DataFrame `rules` la longitud de los antecedentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[\"antecedent_len\"] = rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "rules"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Y ahora usamos pandas para filtrar por los parámetros que nos interesan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[ (rules['antecedent_len'] >= 2) &\n",
    "       (rules['confidence'] > 0.75) &\n",
    "       (rules['lift'] > 1.2) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos filtrar las reglas basándonos en las columnas \"antecedents\" o \"consequents\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules[rules['antecedents'] == {'Eggs', 'Kidney Beans'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejercicios propuestos\n",
    "\n",
    "## 1. Probar otros algoritmos de búsqueda de frequent itemsets\n",
    "\n",
    "La librería mlxtend tiene otros dos algoritmos de búsqueda de frequent itemsets (http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/): FP-Growth (http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpgrowth/) y FP-Max (http://rasbt.github.io/mlxtend/user_guide/frequent_patterns/fpmax/). Prueba ambos algoritmos con el pequeño dataset de prueba, incluyendo la fase de genereación de reglas.\n",
    "\n",
    "## 2. Aplicar los algoritmos a datasets reales (*entrega en Moovi*)\n",
    "\n",
    "En la carpeta `frequent-itemsets` hay dos ficheros con datos reales: `groceries.csv` y `store_data.csv`. Utiliza estos ficheros (uno de ellos o ambos) para desarrollar un caso de estudio con un pequeño análisis de los resultados. Haz esto en un notebook Jupyter separado.\n",
    "\n",
    "A continuación encontrarás un ejemplo de cómo cargar estos ficheros a una lista de listas que podrás utilizar para analizar con mlxtend:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open('groceries.csv')\n",
    "groceries = []\n",
    "for line in fh:\n",
    "    groceries.append(line.replace('\\n', '').split(','))\n",
    "fh.close()\n",
    "\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(groceries).transform(groceries)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
