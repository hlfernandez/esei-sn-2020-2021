{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clasificación utilizando `sklearn`: predicción del estado de un crédito\n",
    "\n",
    "En este notebook vamos entrenar distintos modelos de clasificación para predecir el estado de un crédito utilizando este dataset disponible en Kaggle: https://www.kaggle.com/zaurbegiev/my-dataset\n",
    "\n",
    "Echa un vistazo a las columnas disponibles para entender de qué información se dispone.\n",
    "\n",
    "En primer lugar, cargamos las librerías necesarias y listamos los ficheros del directorio actual para comprobar que estamos bien situados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"./\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos el fichero de datos `credit_train.csv` y mostramos las primeras filas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('credit_train.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como ya es habitual, mostramos información básica acerca de cada dataset utilizando las funciones `info()` y `describe()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: \", data.shape)\n",
    "print('**'* 50)\n",
    "data.info()\n",
    "print('**'* 50)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de datos\n",
    "\n",
    "### Limpieza y comprobación de valores perdidos\n",
    "\n",
    "En primer lugar, vemos que Las columnas `Loan ID`y `Customer ID` simplemente son para identificación y no se utilizan para entrenar los modelos, las eliminamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels=['Loan ID', 'Customer ID'], axis=1, inplace=True)\n",
    "print(\"Dataset size: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprobamos cuántos valores nulos hay en cada columna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos una función para devolver esta información ordenada y con el porcentaje sobre el total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_missing_values_table(df):\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    missing_values_percent = 100 * df.isnull().sum() / len(df)\n",
    "\n",
    "    table = pd.concat([missing_values, missing_values_percent], axis=1)\n",
    "\n",
    "    table = table.rename(\n",
    "        columns = {0 : 'Missing Values', 1 : '% of Total Values'}\n",
    "    )\n",
    "\n",
    "    table = table[table.iloc[:,1] != 0].sort_values('% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    return table\n",
    "\n",
    "create_missing_values_table(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay varias casuísticas. En primer lugar, vemos que existe una columna (`Months since last delinquent`) para la cual tenemos valores perdidos en más del 50% de las filas, por lo que la eliminamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 'Months since last delinquent', axis=1, inplace=True)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "display(create_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Llaman la atención las 514 filas de varias columnas que tienen valores perdidos. ¿Será que todas esas filas solo tienen valores perdidos en todas las columnas? Lo comprobamos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['Years of Credit History'].isnull() == True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curiosamente, son las 514 últimas filas del dataset, por lo que podemos eliminarlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.tail(514).index, inplace=True)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "display(create_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tenemos 3 columnas (`Bankruptcies`, `Tax Liens` y `Maximum Open Credit`) con un número muy bajo de filas con valores perdidos sobre el total, por lo que podemos optar por eliminarlas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data['Maximum Open Credit'][data['Maximum Open Credit'].isnull() == True].index:\n",
    "    data.drop(labels=i, inplace=True)\n",
    "\n",
    "for i in data['Tax Liens'][data['Tax Liens'].isnull() == True].index:\n",
    "    data.drop(labels=i, inplace=True)\n",
    "\n",
    "for i in data['Bankruptcies'][data['Bankruptcies'].isnull() == True].index:\n",
    "    data.drop(labels=i, inplace=True)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "display(create_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dos de las columnas que quedan son numéricas (`Credit Score` y `Annual Income`), podemos utilizar la estrategia de rellenar con el valor medio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna(data.mean(), inplace=True)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "display(create_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y finalmente, nos queda la columna `Years in current job`. Veamos cuál es el valor más frecuente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(data['Years in current job'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por lo que, tal y como vimos la semana pasada, podemos asignar el valor más frecuente `10+ years` a los valores perdidos. En el notebook de regresión lo hacíamos utilizando un `SimpleInputer`, aquí podemos hacerlo directamente con `fillna`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('10+ years', inplace=True)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)\n",
    "display(create_missing_values_table(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valores duplicados\n",
    "\n",
    "Una función interesante es `drop_duplicates`, que permite eliminar filas redundandes. Si sospechamos que puede pasar esto en nuestro dataset, aplicamos esta función y vemos los cambios en el tamaño del dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset size: \", data.shape)\n",
    "\n",
    "data.drop_duplicates(inplace = True)\n",
    "\n",
    "print(\"Dataset size without duplicates: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, había un número importante de filas duplicadas, así que nos quedamos con la versión limpia.\n",
    "\n",
    "## Variables categóricas\n",
    "\n",
    "En el dataset tenemos algunas variables categóricas, que es necesario convertir a variables numéricas para poder entrenar los modelos de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(data['Term'])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(data['Years in current job'])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(data['Home Ownership'])\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "sns.countplot(data['Purpose'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizaremos la *one-hot encoding* para transformar estas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_subset = data[['Term', 'Years in current job', 'Home Ownership', 'Purpose']]\n",
    "\n",
    "categorical_subset = pd.get_dummies(categorical_subset)\n",
    "\n",
    "display(categorical_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que la columna `Term` solo tiene dos valores, con el valor de una columna sabemos el de la otra, por lo que podemos eliminar una de ellas. Eliminamos una de ellas y concatenamos la codificación *one-hot* al dataset original, tras haber eliminado las columnas originales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_subset.drop(labels=['Term_Long Term'], axis=1, inplace=True)\n",
    "\n",
    "data.drop(labels=['Term', 'Years in current job', 'Home Ownership', 'Purpose'], axis=1, inplace=True)\n",
    "data = pd.concat([data, categorical_subset], axis = 1)\n",
    "\n",
    "print(\"Dataset size: \", data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualización de datos\n",
    "\n",
    "En este apartado puedes aplicar cualquier técnica de visualización para analizar los datos de una o varias columnas en función de su tipo.\n",
    "\n",
    "Nos fijaremos en la distribución de la columna que queremos predecir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data.shape)\n",
    "display(data['Loan Status'].unique())\n",
    "display(data['Loan Status'].value_counts())\n",
    "\n",
    "plt.figure(figsize=(20,8))\n",
    "\n",
    "sns.countplot(data['Loan Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puedes ver, se trata de un dataset desbalanceado ya que tenemos casi un 75% de muestras `Fully paid`.\n",
    "\n",
    "Esto es importante, porque un modelo que clasificase todas las muestras como `Fully paid`\n",
    "obtendría una tasa de aciertos del 75%.\n",
    "\n",
    "En este notebook usaremos el dataset desbalanceado, pero aquí están algunos recursos sobre estrategias para lidiar con este problema que puedes aplicar una vez terminado el notebook:\n",
    "- https://www.kdnuggets.com/2017/06/7-techniques-handle-imbalanced-data.html\n",
    "- https://elitedatascience.com/imbalanced-classes\n",
    "\n",
    "Utilizaremos una métrica que permita tener en cuenta el desbalanceo como puede ser el F1 score (https://en.wikipedia.org/wiki/F-score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "\n",
    "### División en Train y Validation\n",
    "\n",
    "Cargamos las librerías para este apartado del notebook y separamos el dataset disponible en `X`, un dataframe con las variables predictoras, e `Y`, un dataframe con la variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "random_state = 2020\n",
    "\n",
    "data.ml = data\n",
    "\n",
    "print('Tamaño del dataset de entrenamiento (muestras x variables):', data.ml.shape)\n",
    "\n",
    "X = data.ml.drop(columns='Loan Status')\n",
    "Y = pd.DataFrame(data.ml['Loan Status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recodificamos la variable de salida a 0 y 1. Esto evita problemas con ciertos modelos y métricas que requieren de este tipo de codificación.\n",
    "\n",
    "*Importante*: a la hora de calcular ciertas métricas como `precision` o `recall`, debes saber cuál es la clase positiva (1) y negativa (0) para poder interpretar los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "Y_binary = LabelEncoder().fit_transform(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividiremos el dataset de entrenamiento en tres subcojuntos: 60% de datos para `train`, 20% `validation` y 20% para `test`. Primero dividimos en 80% para `train` (que podemos utilizar para una validación cruzada o dividirlo para una validación *holdout*) y 20% para `test`. El dataset `train` volvemos a dividirlo en dos, de manera que tengamos un 60% para `train` y un 20% para `validation`.\n",
    "\n",
    "Utilizamos `stratify` para que se mantenga la distribución de la clase que queremos predecir en las particiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_binary\n",
    "\n",
    "train_ratio = 0.60\n",
    "test_ratio = 0.20\n",
    "validation_ratio = 0.20\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(\n",
    "    X, Y,\n",
    "    test_size=test_ratio,\n",
    "    stratify=Y,\n",
    "    random_state=2020\n",
    ")\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(\n",
    "    X_train_val, Y_train_val, \n",
    "    test_size=validation_ratio/(test_ratio+train_ratio),\n",
    "    stratify=Y_train_val,\n",
    "    random_state=2020\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación:\n",
    "- Entrenaremos varios modelos utilizando validación *holdout* y validación cruzada. El dataset `test` quedará reservado hasta el final.\n",
    "- Buscaremos los mejores valores de parámetros de varios modelos utilizando validación cruzada.\n",
    "- Compararemos los mejores modelos y escogeremos uno.\n",
    "- Probaremos el modelo escogido en el dataset `test`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo y validación *holdout*\n",
    "\n",
    "Empezamos por entrenar un árbol de decisión, disponible en `DecisionTreeClassifier` (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html). Utilizamos la partición `train` (60%) para entrenar y `validation` (20%) para probar el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt_prediction = dt.predict(X_val)\n",
    "print('Decision Tree accuracy = ', metrics.accuracy_score(dt_prediction, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El árbol de decisión se puede visualizar de varias maneras, tal y como se explica aquí: https://mljar.com/blog/visualize-decision-tree/\n",
    "        \n",
    "    \n",
    "Visualizamos las reglas en modo texto, que es una de las maneras más sencillas. Esto puede tardar un rato, así que ve echándole un vistazo al siguiente apartado.\n",
    "\n",
    "*Disclaimer*: intenté visualizarlas gráficamente pero daba bastantes problemas no sé por qué, así que si queréis pelearos con esto adelante ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "text_representation = tree.export_text(dt, feature_names = data.columns.drop('Loan Status').tolist())\n",
    "print(text_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de un modelo  y validación cruzada\n",
    "\n",
    "Otra manera de evaluar el rendimiento de un modelo es la *K-Fold Cross-Validation*. En esta página de `sklearn` puedes encontrar más información sobre ello: https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "La manera más sencilla es utilizar `cross_val_score`, que permite evaluar una sola métrica. En este ejemplo validaremos el rendimiento de un árbol de decisión utilizando esta técnica para aprender a usar el API. En este caso, utilizamos el 80% de los datos disponible en `X_train_val` (recuerda: es importante es no usar el `test` hasta tener escogido un modelo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "scores = cross_val_score(estimator = dt, X = X_train_val, y = Y_train_val, cv = 5)\n",
    "\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por defecto, el `score` que se calcula es la tasa de aciertos (`accuracy`). Se puede utilizar otra métrica de evaluación (definidas en https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter), en función de las necesidades y de si el dataset está balanceado o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "\n",
    "scores = cross_val_score(estimator = dt, X = X_train_val, y = Y_train_val, cv = 5, scoring = 'f1')\n",
    "\n",
    "print(\"F1: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "scores = cross_val_score(estimator = dt, X = X_train_val, y = Y_train_val, cv = 5, scoring = 'precision')\n",
    "\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, este enfoque no es práctico si queremos evaluar múltiples métricas a la vez. Para ello, se utiliza la función `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "scoring = ['precision', 'accuracy', 'f1']\n",
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "scores = cross_validate(dt, X_train_val, Y_train_val, scoring=scoring)\n",
    "\n",
    "display(sorted(scores.keys()))\n",
    "\n",
    "display(scores['test_f1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entre las métricas de evaluación verás que algunas tienen versiones *macro* y *micro*. Echa un vistazo a este post para entender en qué consiste: http://rushdishams.blogspot.com/2011/08/micro-and-macro-average-of-precision.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio: variar los parámetros del árbol de decisión\n",
    "\n",
    "Como ves, el rendimiento del árbol está por debajo del 75% que obtendríamos si predijésemos la clase mayoritaria. Echa un vistazo a los parámetros del árbol (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) y entrena distintos modelos variando algún parámetro para ver si consigues mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Añade parámetros a DecisionTreeClassifier y prueba distintos valores\n",
    "dt = DecisionTreeClassifier(random_state = random_state)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "dt_prediction = dt.predict(X_val)\n",
    "print('Decision Tree accuracy = ', metrics.accuracy_score(dt_prediction, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Búsqueda de los mejores parámetros\n",
    "\n",
    "En el ejercicio anterior intentábamos buscar parámetros que mejorasen el rendimiento del árbol. Esto se puede hacer de manera automática utilizando `GridSearchCV` (https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Echa un vistazo a la documentación de `sklearn` disponible en esta otra página (https://scikit-learn.org/stable/modules/grid_search.html) antes de ejecutar el siguiente código. Presta atención al `refit` y lo que significa.\n",
    "\n",
    "En este ejemplo creamos un *grid* con los valores de dos parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "dt_parameters = {\n",
    "    'criterion':('gini', 'entropy'),\n",
    "    'max_depth': np.arange(1, 15, 4)\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.gscv = GridSearchCV(dt, dt_parameters, cv=5, scoring='f1')\n",
    "dt.gscv.fit(X_train_val,Y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E imprimimos las variables `best_params_` y `cv_results_` para obtener el resultado de la mejor combinación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.gscv.best_params_)\n",
    "print(dt.gscv.best_estimator_)\n",
    "print(dt.gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sorprendentemente, el mejor resultado se obtiene con una profundidad de 1, lo cual nos puede hacer pensar que quizá el árbol de decisión no tenga la suficiente capacidad de generalización para este dataset concreto. \n",
    "\n",
    "Si quisiéramos seleccionar el mejor modelo y utilizarlo, este estaría en `best_estimator_`. A modo de ejemplo, calculamos la tasa de aciertos del mejor modelo en los datos de validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_prediction = dt.gscv.best_estimator_.predict(X_val)\n",
    "print('Best Decision Tree accuracy = ', metrics.accuracy_score(dt_prediction, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos imprimir todos los valores de resultados obtenidos en la validación cruzada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.gscv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicios\n",
    "\n",
    "Repite los pasos anteriores para distintos modelos:\n",
    "- Random Forest (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
    "- KNN (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "- SVM (https://scikit-learn.org/stable/modules/svm.html).\n",
    "- Logistic Regression (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "\n",
    "Para cada modelo crea un apartado en el notebook y:\n",
    "- Comienza probando un clasificador básico con los parámetros por defecto.\n",
    "- Varía algún parámetro para ver cómo cambia el rendimiento.\n",
    "- Busca la mejor combinación de parámetros con `GridSearchCV`.\n",
    "\n",
    "Algunos consideraciones:\n",
    "- En el caso de la SVM necesitarás escalar los datos, algo que hicimos en notebooks anteriores. Fíjate en que en el ejemplo de SVM (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) se utiliza un `Pipeline`. Esto es necesario porque los datos se deben escalar utilizando la media y la desviación típica partición de entrenamiento de cada fold, de lo contrario se produciría *data leakage* Mira el apartado de lecturas adicionales.\n",
    "- En el caso del Random Forest, puedes intentar extraer la importancia de cada variable y hacer una representación gráfica.\n",
    "- Si algún modelo tarda mucho en entrenarse (o en completar la búsqueda con `GridSearchCV`), puedes probar a reducir el tamaño del dataset (por ejemplo, eliminando todas las filas con valores perdidos al principio).\n",
    "\n",
    "Finalmente, crea un apartado en el notebook para comparar el rendimiento de los distintos modelos (por ejemplo, escogiendo el que mejor F1-score tenga) para escoger el que considieres mejor y aplica este modelo al dataset `test` que tenemos reservado.\n",
    "\n",
    "Si tienes tiempo y ganas, puedes probar otros modelos o alguna de las estrategias de balanceo (over/up-sampling).\n",
    "\n",
    "## Lecturas adicionales\n",
    "\n",
    "El *data leakage* se produce cuando de algún modo se utilizan datos (muestras) de test durante el proceso de entrenamiento. Si aplicamos *feature selection* antes de entrenar un modelo y utilizamos todas las muestras para ello, estaríamos ocasionando *data leakage*. En esta página (http://thatdatatho.com/2018/10/04/cross-validation-the-wrong-way-right-way-feature-selection/) puedes ver un ejemplo. En este ejemplo de `sklearn` (https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.html), puedes ver un pipeline sencillo en que se aplica *Principal Component Analysis* para transformar las variables seguida de una regresión lógistica para hacer la clasificación."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
