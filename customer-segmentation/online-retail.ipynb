{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Segmentation using K-Means: Online Retail Dataset\n",
    "\n",
    "En este caso de estudio se muestran los pasos necesarios para identificar grupos de clientes utilizando el Online Retail Dataset (https://www.kaggle.com/puneetbhaya/online-retail). \n",
    "\n",
    "En este notebook se reproducen (con pequeñas modificaciones) los pasos dados en este Notebook de Kaggle (https://www.kaggle.com/mgmarques/customer-segmentation-and-market-basket-analysis).\n",
    "\n",
    "Comenzamos importando las librerías necesarias y cargando el fichero que contiene los datos de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cs_df = pd.read_excel(io=r'online-retail.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)\n",
    "\n",
    "Es una buena práctica comenzar con un análisis exploratorio de datos para conocer el dataset al que nos enfrentamos e identificar posibles anomalías o situaciones que haya que tener en cuenta para el análisis (valores perdidos, variables que sea necesario escalar, presencia de outliers, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cs_df.head())\n",
    "\n",
    "cs_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos una función (inspirada en `str` de R) que muestra las características generales de cada columna del dataset: número de valores, número de valores nulos, número de valores distintos, porcentaje de valores perdidos, skewness (https://en.wikipedia.org/wiki/Skewness) y kurtosis (https://en.wikipedia.org/wiki/Kurtosis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rstr(df): \n",
    "    obs = df.shape[0]\n",
    "    types = df.dtypes\n",
    "    counts = df.apply(lambda x: x.count())\n",
    "    uniques = df.apply(lambda x: [x.unique()])\n",
    "    nulls = df.apply(lambda x: x.isnull().sum())\n",
    "    distincts = df.apply(lambda x: x.unique().shape[0])\n",
    "    missing_ration = (df.isnull().sum()/ obs) * 100\n",
    "    skewness = df.skew()\n",
    "    kurtosis = df.kurt()\n",
    "    skewness = df.skew()\n",
    "    kurtosis = df.kurt() \n",
    "    \n",
    "    str = pd.DataFrame({\n",
    "        'types': types, \n",
    "        'counts': counts, \n",
    "        'nulls': nulls, \n",
    "        'distincts': distincts, \n",
    "        'missing_ration': missing_ration,\n",
    "        'skewness': skewness,\n",
    "        'kurtosis': kurtosis\n",
    "    })\n",
    "\n",
    "    return str\n",
    "\n",
    "details = rstr(cs_df)\n",
    "display(details.sort_values(by='missing_ration', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que las variables `Quantity` y `UnitPrice` tienen valores negativos, lo cual puede significar que hay transacciones correspondientes a devoluciones. Como queremos segmentar clientes, es importante limpiar el dataset para eliminar las filas correspondientes primero.\n",
    "\n",
    "Vamos a analizar el dataset para ver si existen filas en las que ambos valores sean negativos o si uno es negativo y el otro cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check if we had negative quantity and prices at same register:',\n",
    "     'No' if cs_df[(cs_df.Quantity<0) & (cs_df.UnitPrice<0)].shape[0] == 0 else 'Yes', '\\n')\n",
    "print('Check how many register we have where quantity is negative',\n",
    "      'and prices is 0 or vice-versa:',\n",
    "      cs_df[(cs_df.Quantity<=0) & (cs_df.UnitPrice<=0)].shape[0])\n",
    "print('\\nWhat is the customer ID of the registers above:',\n",
    "      cs_df.loc[(cs_df.Quantity<=0) & (cs_df.UnitPrice<=0), \n",
    "                ['CustomerID']].CustomerID.unique())\n",
    "print('\\n% Negative Quantity: {:3.2%}'.format(cs_df[(cs_df.Quantity<0)].shape[0]/cs_df.shape[0]))\n",
    "print('\\nAll register with negative quantity has Invoice start with:', \n",
    "      cs_df.loc[(cs_df.Quantity<0) & ~(cs_df.CustomerID.isnull()), 'InvoiceNo'].apply(lambda x: x[0]).unique())\n",
    "print('\\nSee an example of negative quantity and others related records:')\n",
    "display(cs_df[(cs_df.CustomerID==12472) & (cs_df.StockCode==22244)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No existen registros en que ambos valores sean negativos. Veamos ejemplo de registros con un precio negativo o cero:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check register with UnitPrice negative:', cs_df[(cs_df.UnitPrice<0)].shape[0])\n",
    "display(cs_df[(cs_df.UnitPrice<0)])\n",
    "\n",
    "print(\"Sales records with Customer ID and zero in Unit Price:\",cs_df[(cs_df.UnitPrice==0)  & ~(cs_df.CustomerID.isnull())].shape[0])\n",
    "display(cs_df[(cs_df.UnitPrice==0)  & ~(cs_df.CustomerID.isnull())])\n",
    "\n",
    "print(\"Sales records without Customer ID:\",cs_df[cs_df.CustomerID.isnull()].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, no hay registros con la cantidad y el precio negativos al mismo tiempo, pero sí que hay 1336 registros donde uno de ellos lo es y el otro es cero. Para estos registros, además, no existe un ID de cliente. Además, hay 135 080 sin ID de cliente en total. Podemos concluir que es necesario eliminar estos registros antes de proceder con el análisis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove register without CustomerID\n",
    "cs_df = cs_df[~(cs_df.CustomerID.isnull())]\n",
    "\n",
    "# Remove negative or return transactions\n",
    "cs_df = cs_df[~(cs_df.Quantity<0)]\n",
    "cs_df = cs_df[cs_df.UnitPrice>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### details = rstr(cs_df)\n",
    "display(details.sort_values(by='distincts', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Añadimos una columna `amount` para guardar la cantidad total gastada y convertimos las fechas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_df.InvoiceDate = pd.to_datetime(cs_df.InvoiceDate)\n",
    "cs_df['amount'] = cs_df.Quantity*cs_df.UnitPrice\n",
    "cs_df.CustomerID = cs_df.CustomerID.astype('Int64')\n",
    "\n",
    "details = rstr(cs_df)\n",
    "display(details.sort_values(by='distincts', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables del modelo RFM\n",
    "\n",
    "### Recency\n",
    "\n",
    "Para crear la variable `recency` debemos establecer una fecha de referencia para el análisis. Normalmente seleccionamos la fecha de la transacción más antigua más un día y luego construimos la variable `recency` contando el número de días de cada cliente desde su última compra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refrence_date = cs_df.InvoiceDate.max() + datetime.timedelta(days = 1)\n",
    "print('Reference Date:', refrence_date)\n",
    "cs_df['days_since_last_purchase'] = (refrence_date - cs_df.InvoiceDate).astype('timedelta64[D]')\n",
    "customer_history_df =  cs_df[['CustomerID', 'days_since_last_purchase']].groupby(\"CustomerID\").min().reset_index()\n",
    "customer_history_df.rename(columns={'days_since_last_purchase':'recency'}, inplace=True)\n",
    "customer_history_df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un histograma con la distribución de esta nueva variable y podemos observar que tiene uan asimetría importante (`skewed right`) y un pico casi al final. En estos casos se puede transformar la variable empleando el logaritmo (u otra transformación) que elimine esta asimetría y haga que la distribución se asemeje más a una normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(customer_history_df['recency'], color = 'blue', edgecolor = 'black', bins = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency\n",
    "\n",
    "Para crear la variable `frequency` simplemente contamos el número de transacciones de cada cliente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_freq = (cs_df[['CustomerID', 'InvoiceNo']].groupby([\"CustomerID\", 'InvoiceNo']).count().reset_index()).\\\n",
    "                groupby([\"CustomerID\"]).count().reset_index()\n",
    "customer_freq.rename(columns={'InvoiceNo':'frequency'},inplace=True)\n",
    "customer_history_df = customer_history_df.merge(customer_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con la variable anterior, si hacemos un histograma con la distribución de esta nueva variable y podemos observar que tiene uan asimetría bastante marcada (`skewed right`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(customer_history_df['frequency'], color = 'blue', edgecolor = 'black', bins = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency\n",
    "\n",
    "Para crear la variable `monetary` simplemente sumamos el valor de `amount` para cada cliente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_monetary_val = cs_df[['CustomerID', 'amount']].groupby(\"CustomerID\").sum().reset_index()\n",
    "customer_history_df = customer_history_df.merge(customer_monetary_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con las variables anteriores, si hacemos un histograma con la distribución de esta nueva variable y podemos observar que tiene uan asimetría bastante marcada (`skewed right`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(customer_history_df['amount'], color = 'blue', edgecolor = 'black', bins = 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostramos el nuevo DataFrame con el modelo RFM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(customer_history_df.head())\n",
    "\n",
    "display(customer_history_df.describe())\n",
    "\n",
    "details = rstr(customer_history_df)\n",
    "display(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesado de datos\n",
    "\n",
    "Ahora que tenemos la matriz con el modelo RFM, vamos a preprocesarla. Por un lado, escalaremos los datos para que todos estén en la misma escala, pues ya hemos visto que K-Means es sensible a variables en distinta escala y en este caso queremos que tengan todas la misma importancia. Además, las variables tienen una distribución asimétrica por la derecha y algunas, especialmente `amount`, toman un rango muy grande de valores. Los redistribuiremos haciendo la transformación logarítmica antes que el escalado.\n",
    "\n",
    "**Importante**: el escalado es reversible, de manera que podemos transformar los centroides del clustering y que sean interpretables en términos del dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_history_df['recency_log'] = customer_history_df['recency'].apply(math.log)\n",
    "customer_history_df['frequency_log'] = customer_history_df['frequency'].apply(math.log)\n",
    "customer_history_df['amount_log'] = customer_history_df['amount'].apply(math.log)\n",
    "feature_vector = ['amount_log', 'recency_log','frequency_log']\n",
    "\n",
    "X_subset = customer_history_df[feature_vector]\n",
    "scaler = preprocessing.StandardScaler().fit(X_subset)\n",
    "\n",
    "X_scaled = scaler.transform(X_subset)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_subset.columns)\n",
    "\n",
    "display(X_scaled_df.describe().T)\n",
    "\n",
    "details = rstr(X_scaled_df)\n",
    "display(details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Comenzamos aplicando el método Elbow y calculando los silhouette scores para tener una idea del mejor valor de k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "\n",
    "k_range = range(2, 20)\n",
    "\n",
    "kmeans = [KMeans(n_clusters=i) for i in k_range]\n",
    "inertia = [kmeans[i].fit(X_scaled).inertia_ for i in range(len(kmeans))]\n",
    "labels_k = [kmeans[i].predict(X_scaled) for i in range(len(kmeans))]\n",
    "silhouettes = [metrics.silhouette_score(X_scaled, labels_k[i], metric = 'euclidean') for i in range(len(kmeans))]\n",
    "\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('inertia')\n",
    "plt.plot(k_range, inertia, linestyle='--', marker='o', color='black')\n",
    "plt.show()\n",
    "\n",
    "silhouettes_df = pd.DataFrame({'k': k_range, 'silhouette': silhouettes}).sort_values('silhouette')\n",
    "\n",
    "display(silhouettes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parece que a partir de 6-7 clusters se frena la reducción de la inercia pero no existe un candidato claro (no se ve tal codo). El menor silhouette se obtiene en k = 7, así que probaremos los valores 3, 5 y 7 (7 porque parece el mejor y los otros dos porque producirán menos grupos que podrían ser más fáciles de interpretar).\n",
    "\n",
    "En este artículo se puede ver una descripción más completa de los Silhouette plots que crearemos a continuación: https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html Estos gráficos muestran las muestras agrupadas y una línea con el valor medio del silhouette. Un k dado sería una mala elección si existen clusters por debajo del silhouette medio (línea roja discontinua) y/o con grandes fluctuaciones en el silhouette score de sus muestras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [3, 5, 7]\n",
    "\n",
    "cluster_centers = dict()\n",
    "\n",
    "for n_clusters in k_values:\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3)\n",
    "    fig.set_size_inches(25, 7)\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(X_scaled) + (n_clusters + 1) * 10])\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, init='k-means++', n_init=10,max_iter=300, tol=1e-04, random_state=101)\n",
    "    cluster_labels = clusterer.fit_predict(X_scaled)\n",
    "\n",
    "    silhouette_avg = metrics.silhouette_score(X = X_scaled, labels = cluster_labels)\n",
    "    cluster_centers.update({n_clusters :{'cluster_center':clusterer.cluster_centers_,\n",
    "                                         'silhouette_score':silhouette_avg,\n",
    "                                         'labels':cluster_labels}\n",
    "                           })\n",
    "\n",
    "    sample_silhouette_values = metrics.silhouette_samples(X = X_scaled, labels = cluster_labels)\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.Spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xticks([-0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])\n",
    "    colors = cm.Spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    \n",
    "    centers = clusterer.cluster_centers_\n",
    "    y = 0\n",
    "    x = 1\n",
    "    ax2.scatter(X_scaled[:, x], X_scaled[:, y], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')   \n",
    "    ax2.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "    ax2.set_title(\"{} Clustered data\".format(n_clusters))\n",
    "    ax2.set_xlabel(feature_vector[x])\n",
    "    ax2.set_ylabel(feature_vector[y])\n",
    "\n",
    "    x = 2\n",
    "    ax3.scatter(X_scaled[:, x], X_scaled[:, y], marker='.', s=30, lw=0, alpha=0.7, c=colors, edgecolor='k')   \n",
    "    ax3.scatter(centers[:, x], centers[:, y], marker='o', c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "    for i, c in enumerate(centers):\n",
    "        ax3.scatter(c[x], c[y], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "    ax3.set_title(\"Silhouette score: {:1.2f}\".format(cluster_centers[n_clusters]['silhouette_score']))\n",
    "    ax3.set_xlabel(feature_vector[x])\n",
    "    ax3.set_ylabel(feature_vector[y])\n",
    "    \n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=14, fontweight='bold')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anállisis de los clusters obtenidos\n",
    "\n",
    "Finalmente, convertimos los centroides de los clusters a los valores originales y procedemos a analizar dichos clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['amount',  'recency',  'frequency']\n",
    "for i in k_values:\n",
    "    print(\"for {} clusters the silhouette score is {:1.2f}\".format(i, cluster_centers[i]['silhouette_score']))\n",
    "    print(\"Centers of each cluster:\")\n",
    "    cent_transformed = scaler.inverse_transform(cluster_centers[i]['cluster_center'])\n",
    "    print(pd.DataFrame(np.exp(cent_transformed),columns=features))\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por ejemplo, en el caso de k = 3, identificamos:\n",
    "- Que el cluster 2 incluye a clientes que compran con bastante frecuencia y gastan una cantidad considerable.\n",
    "- Que los clusters 0 y 1 incluyen a dos segmentos de clientes que compran con poca frecuencia y gastan menos.\n",
    "\n",
    "En el kaso de k = 7 se identifica un grupo muy interesante, el que corresponde al grupo 4: clientes que gastan una cantidad considerable, con mucha frecuencia y cuya última transacción es muy reciente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para terminar el análisis, vamos a añadir las etiquetas generadas en cada agrupamiento al DataFrame que teníamos y examinar los grupos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_history_df['clusters_3'] = cluster_centers[3]['labels'] \n",
    "customer_history_df['clusters_5'] = cluster_centers[5]['labels']\n",
    "customer_history_df['clusters_7'] = cluster_centers[7]['labels']\n",
    "display(customer_history_df.head())\n",
    "\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "f1 = fig.add_subplot(131)\n",
    "market = customer_history_df.clusters_3.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('3 Clusters')\n",
    "f1 = fig.add_subplot(132)\n",
    "market = customer_history_df.clusters_5.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('5 Clusters')\n",
    "f1 = fig.add_subplot(133)\n",
    "market = customer_history_df.clusters_7.value_counts()\n",
    "g = plt.pie(market, labels=market.index, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "plt.title('7 Clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos hacer diagramas de cajas de cada variable para ver cómo se distribuyen, además de imprimir algunas estadísticas de cada agrupamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = 'clusters_3'\n",
    "\n",
    "customer_history_df.boxplot(column=['frequency'], by=clustering)\n",
    "customer_history_df.boxplot(column=['recency'], by=clustering)\n",
    "customer_history_df.boxplot(column=['amount'], by=clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(customer_history_df.groupby([clustering]).mean())\n",
    "display(customer_history_df.groupby([clustering]).std())\n",
    "display(customer_history_df.groupby([clustering])['amount'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
